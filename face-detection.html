<!DOCTYPE html>
<html>

<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title></title>
    <meta name="description" content="">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <style>
        @use "@material";

        body {
            font-family: roboto;
            margin: 2em;
            color: #3d3d3d;
            --mdc-theme-primary: #007f8b;
            --mdc-theme-on-primary: #f1f3f4;
        }

        h1 {
            color: #007f8b;
        }

        h2 {
            clear: both;
        }

        em {
            font-weight: bold;
        }

        video {
            clear: both;
            display: block;
            transform: rotateY(180deg);
            -webkit-transform: rotateY(180deg);
            -moz-transform: rotateY(180deg);
        }

        section {
            opacity: 1;
            transition: opacity 500ms ease-in-out;
        }

        .mdc-button.mdc-button--raised.removed {
            display: none;
        }

        .invisible {
            opacity: 0.2;
        }

        .videoView,
        .detectOnClick {
            position: relative;
            float: left;
            width: 48%;
            margin: 2% 1%;
            cursor: pointer;
        }

        .detectOnClick p {
            position: absolute;
            padding: 5px;
            background-color: #007f8b;
            color: #fff;
            border: 1px dashed rgba(255, 255, 255, 0.7);
            z-index: 2;
            font-size: 12px;
            margin: 0;
        }

        .videoView p {
            position: absolute;
            padding-bottom: 5px;
            padding-top: 5px;
            background-color: #007f8b;
            color: #fff;
            border: 1px dashed rgba(255, 255, 255, 0.7);
            z-index: 2;
            font-size: 12px;
            margin: 0;
        }

        .highlighter {
            background: rgba(0, 255, 0, 0.25);
            border: 1px dashed #fff;
            z-index: 1;
            position: absolute;
        }

        .detectOnClick {
            z-index: 0;
        }

        .detectOnClick img {
            width: 100%;
        }

        .key-point {
            position: absolute;
            z-index: 1;
            width: 3px;
            height: 3px;
            background-color: #ff0000;
            /* border: 1px solid #ffffff; */
            border-radius: 50%;
            display: block;
        }

        .emoji-point {
            position: absolute;
            top: -6px;
            left: -6px;
        }

        .play-video {
            background-color: green;
        }

        .pause-video {
            background-color: red;
        }
    </style>
</head>


<body>
    <link href="https://unpkg.com/material-components-web@latest/dist/material-components-web.min.css" rel="stylesheet">
    <script src="https://unpkg.com/material-components-web@latest/dist/material-components-web.min.js"></script>

    <h1 id="heading">Face detection using the MediaPipe Face Detector task</h1>

    <section id="demos" class="invisible">
        <h2>Demo: Webcam continuous face detection</h2>
        <p>Detect faces from your webcam. When ready click "enable webcam" below and accept access to the webcam.</p>
        <div> <button id="webcamButton" class="mdc-button mdc-button--raised">
                <span class="mdc-button__ripple"></span>
                <span class="mdc-button__label">ENABLE WEBCAM</span>
            </button></div>

        <div id="liveView" class="videoView">

            <video id="webcam" autoplay playsinline></video>
        </div>
    </section>
    <script type="module">
        import * as faceDetection from "https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@0.10.0";
        const {
            FaceDetector,
            FilesetResolver,
            Detection
        } = faceDetection

        const headingEl = document.getElementById("heading");
        const demosSection = document.getElementById("demos");

        let webcamRunning = false;
        let faceDetector;
        let runningMode = "IMAGE";

        // Initialize the object detector
        const initializefaceDetector = async () => {
            const vision = await FilesetResolver.forVisionTasks(
                "https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@0.10.0/wasm"
            );
            faceDetector = await FaceDetector.createFromOptions(vision, {
                baseOptions: {
                    modelAssetPath: `https://storage.googleapis.com/mediapipe-models/face_detector/blaze_face_short_range/float16/1/blaze_face_short_range.tflite`,
                    delegate: "GPU"
                },
                runningMode: runningMode
            });
            demosSection.classList.remove("invisible");
        };
        initializefaceDetector();

        /********************************************************************
         // Demo 2: Continuously grab image from webcam stream and detect it.
         ********************************************************************/

        let video = document.getElementById("webcam");
        const liveView = document.getElementById("liveView");
        let enableWebcamButton;

        // Check if webcam access is supported.
        const hasGetUserMedia = () => !!navigator.mediaDevices?.getUserMedia;

        // Keep a reference of all the child elements we create
        // so we can remove them easilly on each render.
        var children = [];

        // If webcam supported, add event listener to button for when user
        // wants to activate it.
        if (hasGetUserMedia()) {
            enableWebcamButton = document.getElementById("webcamButton");
            enableWebcamButton.addEventListener("click", enableCam);
        } else {
            console.warn("getUserMedia() is not supported by your browser");
        }

        // Enable the live webcam view and start detection.
        async function enableCam(event) {
            if (!faceDetector) {
                alert("Face Detector is still loading. Please try again..");
                return;
            }

            if (webcamRunning === true) {
                webcamRunning = false;
                enableWebcamButton.innerText = "ENABLE PREDICTIONS";
            } else {
                webcamRunning = true;
                enableWebcamButton.innerText = "DISABLE PREDICTIONS";
            }

            // getUsermedia parameters
            const constraints = {
                video: true
            };

            // Activate the webcam stream.
            navigator.mediaDevices
                .getUserMedia(constraints)
                .then(function (stream) {
                    video.srcObject = stream;
                    video.addEventListener("loadeddata", predictWebcam);
                })
                .catch((err) => {
                    console.error(err);
                });
        }

        let lastVideoTime = -1;
        async function predictWebcam() {
            // if image mode is initialized, create a new classifier with video runningMode
            if (runningMode === "IMAGE") {
                runningMode = "VIDEO";
                await faceDetector.setOptions({ runningMode: "VIDEO" });
            }
            let startTimeMs = performance.now();

            // Detect faces using detectForVideo
            if (video.currentTime !== lastVideoTime) {
                lastVideoTime = video.currentTime;
                const detections = faceDetector.detectForVideo(video, startTimeMs)
                    .detections;
                displayVideoDetections(detections);
                controlBackground(detections)
            }

            if (webcamRunning === true) {
                // Call this function again to keep predicting when the browser is ready
                window.requestAnimationFrame(predictWebcam);
            }
        }
        function controlBackground(detections) {
            for (let detection of detections) {
                const { keypoints } = detection;
                // right eye, left eye, nose, mouth, right ear, left ear (from the user's 1st person view perspective)
                const [
                    rightEye,
                    leftEye,
                    nose,
                    mouth,
                    rightEar,
                    leftEar,
                ] = keypoints;
                // Y coordinates are inverted. the nose is actually above the ear when looking up but the data is inverted,s
                const lookingUp = nose.y < rightEar.y || nose.y < leftEar.y;
                const leftSideLookingDown = leftEye.y > leftEar.y;
                const rightSideLookingDown = rightEye.y > rightEar.y;
                if (lookingUp || (leftSideLookingDown && rightSideLookingDown)) {
                    headingEl.classList.add('pause-video')
                } else {
                    headingEl.classList.remove('pause-video')
                }
            }
        }

        function displayVideoDetections(detections) {
            // Remove any highlighting from previous frame.
            for (let child of children) {
                liveView.removeChild(child);
            }
            children.splice(0);

            // Iterate through predictions and draw them to the live view
            for (let detection of detections) {
                const p = document.createElement("p");
                p.innerText =
                    "Confidence: " +
                    Math.round(parseFloat(detection.categories[0].score) * 100) +
                    "% .";
                p.style =
                    "left: " +
                    (video.offsetWidth -
                        detection.boundingBox.width -
                        detection.boundingBox.originX) +
                    "px;" +
                    "top: " +
                    (detection.boundingBox.originY - 30) +
                    "px; " +
                    "width: " +
                    (detection.boundingBox.width - 10) +
                    "px;";

                const highlighter = document.createElement("div");
                highlighter.setAttribute("class", "highlighter");
                highlighter.style =
                    "left: " +
                    (video.offsetWidth -
                        detection.boundingBox.width -
                        detection.boundingBox.originX) +
                    "px;" +
                    "top: " +
                    detection.boundingBox.originY +
                    "px;" +
                    "width: " +
                    (detection.boundingBox.width - 10) +
                    "px;" +
                    "height: " +
                    detection.boundingBox.height +
                    "px;";

                liveView.appendChild(highlighter);
                liveView.appendChild(p);

                // Store drawn objects in memory so they are queued to delete at next call
                children.push(highlighter);
                children.push(p);
                let i = 0;
                for (let keypoint of detection.keypoints) {
                    const emojiEl = document.createElement("span")
                    // right eye, left eye, nose, mouth, right ear, left ear (from the user's 1st person view perspective)
                    switch (i) {
                        case 0:
                            emojiEl.innerText = '👁️'
                            break
                        case 1:
                            emojiEl.innerText = '👁️'
                            break
                        case 2:
                            emojiEl.innerText = '👃'
                            break
                        case 3:
                            emojiEl.innerText = '👄'
                            break
                        case 4:
                            emojiEl.innerText = '👂'
                            break
                        case 5:
                            emojiEl.innerText = '👂'
                            break
                    }
                    i += 1
                    emojiEl.className = "emoji-point";
                    const keypointEl = document.createElement("span");
                    keypointEl.className = "key-point";
                    keypointEl.style.top = `${keypoint.y * video.offsetHeight - 3}px`;
                    keypointEl.style.left = `${video.offsetWidth - keypoint.x * video.offsetWidth - 3
                        }px`;
                    keypointEl.appendChild(emojiEl)
                    liveView.appendChild(keypointEl);
                    children.push(keypointEl);
                }
            }
        }

    </script>
</body>

</html>